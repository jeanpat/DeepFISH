{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook aim:\n",
    "   * Load datafrom 82146 dataset\n",
    "   * Write a dataloader with image standardization\n",
    "   * train an instance segmentation algo:\n",
    "       * jan 2024: try a segformer based on pytorch lightning with the help of bard/chatgpt 3.5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install pycocotools"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install git+git://github.com/airctic/icedata.git --upgrade"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!conda update -n base -c conda-forge conda -y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!conda install h5py -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c huggingface transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fastai.vision import *\n",
    "import os, sys\n",
    "#import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import h5py\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#import pytorch_lightning as pl\n",
    "#import torch.optim as optim\n",
    "#from pytorch_lightning import LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#import torch.nn.functional as F\n",
    "#import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerForSemanticSegmentation\n",
    "#from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='NVIDIA GeForce GTX 960', major=5, minor=2, total_memory=4030MB, multi_processor_count=8)\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_properties(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2\n",
      "_CudaDeviceProperties(name='NVIDIA GeForce GTX 960', major=5, minor=2, total_memory=4030MB, multi_processor_count=8)\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.get_device_properties(0))\n",
    "#print(pl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processor\t: 0\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 26\n",
      "model name\t: Intel(R) Xeon(R) CPU           X5570  @ 2.93GHz\n",
      "stepping\t: 5\n",
      "microcode\t: 0x1d\n",
      "cpu MHz\t\t: 1596.000\n",
      "cache size\t: 8192 KB\n",
      "physical id\t: 0\n",
      "siblings\t: 8\n",
      "core id\t\t: 0\n",
      "cpu cores\t: 4\n",
      "apicid\t\t: 0\n",
      "initial apicid\t: 0\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 11\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm dca sse4_1 sse4_2 popcnt lahf_lm pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid dtherm ida flush_l1d\n",
      "vmx flags\t: vnmi preemption_timer invvpid ept_x_only flexpriority tsc_offset vtpr mtf vapic ept vpid\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit mmio_unknown\n",
      "bogomips\t: 5852.43\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 40 bits physical, 48 bits virtual\n",
      "power management:\n"
     ]
    }
   ],
   "source": [
    "#info regarding cpu cores (only flags)\n",
    "!awk '{if ($0==\"\") exit; print $0}' /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r--  1 jeanpat jeanpat 352413073 juil. 27 11:33 82146_chromosomes_compressed.npz\n",
      "-rw-rw-r--  1 jeanpat jeanpat  10626293 juil. 22  2023 82146_masks_compressed.npz\n"
     ]
    }
   ],
   "source": [
    "!ls -l ./|grep npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display some a pair of image groundtruth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load cleaned masks and chromosomes as two separated arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_data = np.load('./82146_masks_compressed.npz')\n",
    "masks = c_data['x']\n",
    "chroms = np.load('./82146_chromosomes_compressed.npz')\n",
    "images = chroms['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82146, 52, 52) (82146, 52, 52, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 51.5, 51.5, -0.5)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxsAAAC8CAYAAAAQL7MCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAJlUlEQVR4nO3dSW5TSxQG4PIjCUlIIgRRBkCEBKwAiSWwBNZDGLIfNsCMCQMkJCYIgoJoQhdCWtLcN3h6RV0TGwM+cXO/b3Sc3MKFsbF+1blVraqqqgQAANBn/wx6AgAAwHgSNgAAgBDCBgAAEELYAAAAQggbAABACGEDAAAIIWwAAAAhhA0AACCEsAEAAISY6PXCVqsVOQ/4a1VVDXoKqdVaGfQUoKuqWhn0FHyfMPSG4fskJZ8Vhl8vnxUrGwAAQAhhAwAACCFsAAAAIYQNAAAghLABAACEEDYAAIAQwgYAABBC2AAAAEIIGwAAQAhhAwAACCFsAAAAIYQNAAAghLABAACEEDYAAIAQwgYAABBC2AAAAEIIGwAAQAhhAwAACCFsAAAAIYQNAAAghLABAACEEDYAAIAQwgYAABBC2AAAAEIIGwAAQAhhAwAACCFsAAAAIYQNAAAghLABAACEEDYAAIAQwgYAABBC2AAAAEIIGwAAQAhhAwAACCFsAAAAIYQNAAAghLABAACEEDYAAIAQwgYAABBC2AAAAEIIGwAAQAhhAwAACCFsAAAAIYQNAAAghLABAACEEDYAAIAQwgYAABBC2AAAAEIIGwAAQAhhAwAACDEx6AnQH61WK9fz8/O53t/fr13X/hia5G6619N199Ld4JnAaKiKutXxKoDOrGwAAAAhhA0AACCENqpfWFxczPXh4WGuNzY2BjCbuoWFhVxfuHAh1+fOnct12VKVUkqPHj2KnxiN06k9aRjakXptnQL+U/X4c21VnAatfKPPygYAABBC2AAAAEK0qqrqtGJav7DVjMWrGzdu1B5funQp11tbW7menp7O9erqam3M+vp6rss2pu/fv+e6bHtKKaXt7e0T6zNnzuR6aWmpNqZso5qdnc311NTUic+ZUkrv3r3rOO9R1+NbOVSrtTLoKZyKJrUmDUMrWD9V1cqgp9CY75PT0u1/vk6vdL//txy3f9Fh+D5JqbmflT959Zv5Sg1eL58VKxsAAEAIYQMAAAghbAAAACFsfdum/b6I8l6Isp6cnMx1+/ayHz9+zPXZs2dzPTHx4+U+Pj6ujSm31S3r8t6QmZmZ2piyT64cU/68fP6U6n+/cbtnA6Apeu1pH447D6C7frxPbZE7vKxsAAAAIYQNAAAghDaqlNL169dz3d52tLy8nOubN2/m+tmzZ7lub296+vRprv/550eeK1un2rekbX/8v3Lr23JL2/Y/uxxf1uU17XO4du1arl+8eHHi88P/mrTdban8e4/bNrgMTqe2kW7tH8PcEqWFhd81zO9n+svKBgAAEELYAAAAQmijSikdHR11/N3t27dzfefOnVzfv38/11++fKmNKU8HL3emKtubDg4OamPK9qaydaqcW3urVXldOb4c8+3bt9qYvb29XGudgl/TOkW/9NI2MqqtJVqngE6sbAAAACGEDQAAIIQ2qtS9jerBgwe5Lneg2trayvWHDx9qY96+fZvr8lC+ubm5XO/s7NTGlAfxddpZ6uvXr7Uxm5ubJ86n1fqxoL22tpaA7rRKwa9plaKfyvdTP9oHvT+Hl5UNAAAghLABAACEEDYAAIAQjb1nozxZ+/Lly7leXFysXTc/P5/rq1ev5np1dTXX79+/r42ZnJzMdXki+cTEj5e7vEcjpfr9GGW9vb2d6/IejZTqW+62b6UL/TBup4a7NwNOpt+d0zaq2zzz+6xsAAAAIYQNAAAgRGPbqMpTvpeXl3O9tLRUu67cRrb8XXky95MnT2pjym1td3d3c12e8r2/v18bU7ZOlaeOl61T7SeIAz/TKsWw6vdWn3/z/ACnxcoGAAAQQtgAAABCNLaN6uLFi7memprKdblLVUr13aQeP36c6zdv3uT606dPtTGvXr068TnL1qu9vb2Oc2s/XRz4mXYpRlmnlqr2Vqde2q26jdE6xTCxA1UzWdkAAABCCBsAAECIRrVRlS1SMzMzuS53iTo8PKyNKR9vbGzk+vnz57l++fJlbcza2tpfzxUG5bQO8tMGBf/p1ur0J21QWqeAYWJlAwAACCFsAAAAIYQNAAAgRGPv2ShP4y5P+S63wU0ppaqqTryuvH/DVrUAAPAzKxsAAEAIYQMAAAjRqDaq9m1tT/p52SqVUkpHR0e53t7eznXZhlX+HEZduSXtaW2DCwC/wxbPo8PKBgAAEELYAAAAQjSqjWp6ejrX5QniZavU/v5+bczExMSJ15WtV3t7e32dJwxSVOuUE8MBoHmsbAAAACGEDQAAIESj2qgWFhZyXbZUlQf3la1SKaV0fHx84nVapwAATo8dqEaTlQ0AACCEsAEAAIQQNgAAgBCNumfj/PnzuS7v35icnMz1wcFBx/HldrefP3/u7+QAAMZM9etLGHNWNgAAgBDCBgAAEGKs26impqY6Pt7c3Mx12VLVTbnd7ezsbK63trb+dIowcFEnhgNAP5UtWbbBHR1WNgAAgBDCBgAAEGKs26jK079TSunWrVu53t3dzfXr169zXe5M1a7cgWp9fb0fU4Sxdi/dHfQUABgTWqdGk5UNAAAghLABAACEGOs2qvIQvpRSevjwYa5nZmZyPTHx42WYm5urjTk6Osr1zs5Ov6cIA9fe6mR3KgD6pWx9csBfM1nZAAAAQggbAABACGEDAAAI0aqqqqcWularGRuOXblypePvyi1yGT49vpVDtVorg57Cqeh2X4ftbodbVa0MegqN+T5hdA3D90lKzfmsdHu1m/EKjK5ePitWNgAAgBDCBgAAEEIbFWNjGJa9m9JGxejSRgW/NgzfJyn5rDD8tFEBAAADI2wAAAAhhA0AACCEsAEAAIQQNgAAgBDCBgAAEELYAAAAQggbAABACGEDAAAIIWwAAAAhhA0AACCEsAEAAIQQNgAAgBDCBgAAEELYAAAAQggbAABACGEDAAAIIWwAAAAhhA0AACCEsAEAAIQQNgAAgBDCBgAAEELYAAAAQggbAABACGEDAAAIIWwAAAAhhA0AACCEsAEAAIQQNgAAgBDCBgAAEELYAAAAQggbAABACGEDAAAIIWwAAAAhhA0AACCEsAEAAIQQNgAAgBDCBgAAEELYAAAAQggbAABACGEDAAAIIWwAAAAhhA0AACCEsAEAAIQQNgAAgBDCBgAAEELYAAAAQggbAABACGEDAAAIIWwAAAAhhA0AACBEq6qqatCTAAAAxo+VDQAAIISwAQAAhBA2AACAEMIGAAAQQtgAAABCCBsAAEAIYQMAAAghbAAAACGEDQAAIMS/w1H+aFHVggcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 13841\n",
    "im = images[N,:,:]\n",
    "#gt = dataset[N,:,:,1]\n",
    "mask1, mask2 = masks[N,:,:,0], masks[N,:,:,1]\n",
    "mask = im > 0\n",
    "print(images.shape, masks.shape)\n",
    "\n",
    "#plt.Figure(figsize=(15,15))\n",
    "\n",
    "plt.subplot(141)\n",
    "plt.imshow(im, cmap=plt.cm.gray); plt.axis('off')\n",
    "#plt.imshow(images[50,:,:], cmap=plt.cm.gray); plt.axis('off')\n",
    "\n",
    "plt.subplot(142)\n",
    "plt.imshow(im > 1, interpolation= \"nearest\", cmap=plt.cm.jet); plt.axis('off')\n",
    "\n",
    "plt.subplot(143)\n",
    "plt.imshow(mask1, interpolation= \"nearest\", cmap=plt.cm.flag_r); plt.axis('off')\n",
    "\n",
    "plt.subplot(144)\n",
    "plt.imshow(mask2, interpolation= \"nearest\", cmap=plt.cm.flag_r); plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, masks, transform=None):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        #self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        mask = self.masks[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, mask\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_images, train_masks, val_images, val_masks, test_images, test_masks, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.train_images = train_images\n",
    "        self.train_masks = train_masks\n",
    "        self.val_images = val_images\n",
    "        self.val_masks = val_masks\n",
    "        self.test_images = test_images\n",
    "        self.test_masks = test_masks\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        train_dataset = CustomDataset(self.train_images, self.train_masks, transform=self.transform)\n",
    "        return DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_dataset = CustomDataset(self.val_images, self.val_masks, transform=self.transform)\n",
    "        return DataLoader(val_dataset, batch_size=self.batch_size)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        test_dataset = CustomDataset(self.test_images, self.test_masks, transform=self.transform)\n",
    "        return DataLoader(test_dataset, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class ChromDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_images, data_masks_1, data_masks_2):\n",
    "        \"\"\"get three numpys arrays:images, masks1,masks2\n",
    "         assume the three have the same shape.\n",
    "         for this implementaion (82146,52,52)\n",
    "        \"\"\"\n",
    "        self.im_data = data_images\n",
    "        self.masks_1 = data_masks_1\n",
    "        self.masks_2 = data_masks_2\n",
    "        print(self.im_data.shape)\n",
    "        print(self.masks_1.shape)\n",
    "        print(self.masks_2.shape)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.im_data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"return a standardized image and its two associated masks\n",
    "        \"\"\"\n",
    "        image = self.im_data[idx,:,:] \n",
    "        \n",
    "        cmask = np.logical_or(self.masks_1[idx,:,:],self.masks_2[idx,:,:])\n",
    "        \n",
    "        # Calculate mean and std only from masked regions\n",
    "        mean = image[cmask].mean() \n",
    "        std = image[cmask].std()\n",
    "        #normalization\n",
    "        image = (image-mean)/std\n",
    "        \n",
    "        # Convert image and masks to PyTorch tensors\n",
    "        image = torch.from_numpy(image).float()\n",
    "        cmask1 = torch.from_numpy(self.masks_1[idx,:,:]).float()#why float? is there some int or binary?\n",
    "        cmask2 = torch.from_numpy(self.masks_2[idx,:,:]).float() \n",
    "        \n",
    "        \n",
    "        return image, cmask1, cmask2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82146, 52, 52)\n",
      "(82146, 52, 52)\n",
      "(82146, 52, 52)\n",
      "82146\n",
      "torch.Size([52, 52])\n"
     ]
    }
   ],
   "source": [
    "datachrom = ChromDataset(images, masks[:,:,:,0],masks[:,:,:,1])\n",
    "print(datachrom.__len__())\n",
    "I,M1,M2 = datachrom.__getitem__(41093)\n",
    "print(I.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([16, 52, 52])\n"
     ]
    }
   ],
   "source": [
    "# Create the dataloader\n",
    "dataloader = DataLoader(datachrom, batch_size=16, shuffle=True)\n",
    "\n",
    "# Iterate over the dataloader\n",
    "for images, masks_1, masks_2 in dataloader:\n",
    "    # Do something with the images and masks the images and masks\n",
    "    print(type(images), images.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## send to a segformer Bar Gemini version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "#from torch.utils.data import DataLoader\n",
    "#from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "# Dataset setup\n",
    "#dataset = YourDataset(images_path, masks1_path, masks2_path)  # Replace with your dataset class\n",
    "\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(datachrom, [65717, 8214, 8215])  # Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample0 = train_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 51.5, 51.5, -0.5)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(131)\n",
    "plt.imshow(sample0[0], cmap=plt.cm.gray); plt.axis('off')\n",
    "#plt.imshow(images[50,:,:], cmap=plt.cm.gray); plt.axis('off')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.imshow(sample0[1], interpolation= \"nearest\", cmap=plt.cm.jet); plt.axis('off')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.imshow(sample0[2], interpolation= \"nearest\", cmap=plt.cm.flag_r); plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=16)\n",
    "test_loader = DataLoader(test_set, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerConfig"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Load the SegFormer model\n",
    "config = SegformerConfig.from_pretrained(\"nvidia/mit-b5\", num_channels=16)\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(config)  # Load a pre-trained model \"nvidia/mit-b5\"\n",
    "#config.num_channels = 16\n",
    "\n",
    "# Customize model configuration for 16 channels\n",
    "#config = SegformerConfig.from_pretrained(\"nvidia/mit-b5\", num_channels=16)\n",
    "#model = SegformerForSemanticSegmentation.from_config(config)  # Instantiate from config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 7, 7], expected input[1, 16, 52, 52] to have 3 channels, but got 16 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks1, masks2 \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      4\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 5\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, masks1) \u001b[38;5;241m+\u001b[39m criterion(outputs, masks2)  \u001b[38;5;66;03m# Combine losses for both masks\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/mnt/stockage/Developp/Trans/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/stockage/Developp/Trans/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/stockage/Developp/Trans/lib/python3.9/site-packages/transformers/models/segformer/modeling_segformer.py:793\u001b[0m, in \u001b[0;36mSegformerForSemanticSegmentation.forward\u001b[0;34m(self, pixel_values, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    788\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    789\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    790\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    791\u001b[0m )\n\u001b[0;32m--> 793\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msegformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# we need the intermediate hidden states\u001b[39;49;00m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    800\u001b[0m encoder_hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mhidden_states \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    802\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_head(encoder_hidden_states)\n",
      "File \u001b[0;32m/mnt/stockage/Developp/Trans/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/stockage/Developp/Trans/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/stockage/Developp/Trans/lib/python3.9/site-packages/transformers/models/segformer/modeling_segformer.py:550\u001b[0m, in \u001b[0;36mSegformerModel.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    545\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    546\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    547\u001b[0m )\n\u001b[1;32m    548\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 550\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m/mnt/stockage/Developp/Trans/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/stockage/Developp/Trans/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/stockage/Developp/Trans/lib/python3.9/site-packages/transformers/models/segformer/modeling_segformer.py:424\u001b[0m, in \u001b[0;36mSegformerEncoder.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    422\u001b[0m embedding_layer, block_layer, norm_layer \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# first, obtain patch embeddings\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m hidden_states, height, width \u001b[38;5;241m=\u001b[39m \u001b[43membedding_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# second, send embeddings through blocks\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(block_layer):\n",
      "File \u001b[0;32m/mnt/stockage/Developp/Trans/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/stockage/Developp/Trans/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/stockage/Developp/Trans/lib/python3.9/site-packages/transformers/models/segformer/modeling_segformer.py:139\u001b[0m, in \u001b[0;36mSegformerOverlapPatchEmbeddings.forward\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pixel_values):\n\u001b[0;32m--> 139\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     _, _, height, width \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# (batch_size, num_channels, height, width) -> (batch_size, num_channels, height*width) -> (batch_size, height*width, num_channels)\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# this can be fed to a Transformer layer\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/stockage/Developp/Trans/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/stockage/Developp/Trans/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/stockage/Developp/Trans/lib/python3.9/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/stockage/Developp/Trans/lib/python3.9/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[1, 16, 52, 52] to have 3 channels, but got 16 channels instead"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(2):\n",
    "    for images, masks1, masks2 in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks1) + criterion(outputs, masks2)  # Combine losses for both masks\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for images, masks1, masks2 in val_loader:\n",
    "            outputs = model(images)\n",
    "            val_loss += criterion(outputs, masks1) + criterion(outputs, masks2)\n",
    "    print(f\"Epoch {epoch}, Validation Loss: {val_loss.item() / len(val_loader)}\")\n",
    "\n",
    "# Testing loop (similar to validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset\n",
    "split into train, validate and test\n",
    "### Code from Bard +Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "# Dataset setup\n",
    "##dataset = YourDataset(images_path, masks1_path, masks2_path)  # Replace with your dataset class\n",
    "\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(datachrom, [65717, 8214, 8215])  # Split dataset\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=16)\n",
    "test_loader = DataLoader(test_set, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SegFormer model\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b5\")  # Load a pre-trained model\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    for images, masks1, masks2 in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks1) + criterion(outputs, masks2)  # Combine losses for both masks\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for images, masks1, masks2 in val_loader:\n",
    "            outputs = model(images)\n",
    "            val_loss += criterion(outputs, masks1) + criterion(outputs, masks2)\n",
    "    print(f\"Epoch {epoch}, Validation Loss: {val_loss.item() / len(val_loader)}\")\n",
    "\n",
    "# Testing loop (similar to validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training code by chatgpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Préparation des données :\n",
    "Assure-toi que tes données d'entraînement sont organisées correctement. Tu devras probablement utiliser un DataLoader pour charger tes lots de données.\n",
    "\n",
    "Création du modèle SegFormer :\n",
    "Utilise la classe ViTFeatureExtractor de la bibliothèque Transformers pour extraire les caractéristiques des images, puis utilise ces caractéristiques pour construire ton modèle SegFormer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "import torch\n",
    "\n",
    "# Charger le feature extractor\n",
    "feature_extractor = ViTFeatureExtractor(model_name=\"your_pretrained_model_name\")\n",
    "\n",
    "# Créer le modèle SegFormer\n",
    "model = ViTForImageClassification.from_pretrained(\"your_pretrained_model_name\")\n",
    "\n",
    "# Modifier la tête de classification pour l'adaptation à la segmentation\n",
    "model.classifier = torch.nn.Conv2d(in_channels=model.config.hidden_size, out_channels=num_classes, kernel_size=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entraînement du modèle :\n",
    "Entraîne ton modèle avec tes données d'entraînement. Utilise une fonction de perte adaptée à la segmentation, comme la perte de segmentation croisée (torch.nn.CrossEntropyLoss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Définir l'optimiseur\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Définir la fonction de perte\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Boucle d'entraînement\n",
    "for epoch in range(num_epochs):\n",
    "    for images, masks in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segformer by claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset \n",
    "dataset = load_dataset('my_dataset') \n",
    "\n",
    "# Split into train, val, test\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation'] \n",
    "test_dataset = dataset['test']\n",
    "\n",
    "# Define model\n",
    "model = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-ade-512-512')\n",
    "\n",
    "# Send model to GPU\n",
    "model.to('cuda') \n",
    "\n",
    "# Define optimizer \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(10):\n",
    "  \n",
    "  for batch in train_dataset:\n",
    "    \n",
    "    # Get input images and target masks\n",
    "    images = batch['images'].to('cuda')  \n",
    "    mask1 = batch['mask1'].to('cuda')\n",
    "    mask2 = batch['mask2'].to('cuda')\n",
    "    \n",
    "    # Forward pass   \n",
    "    outputs = model(images)  \n",
    "    \n",
    "    # Compute loss \n",
    "    loss1 = loss_fn(outputs[0], mask1) \n",
    "    loss2 = loss_fn(outputs[1], mask2)\n",
    "    loss = loss1 + loss2\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "  # Validation loop\n",
    "  # Calculate metrics\n",
    "  # Print results\n",
    "\n",
    "# Test loop \n",
    "# Make predictions\n",
    "# Calculate metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definissons un algo ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTSegmentation(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, image_size=(52, 52), num_classes=10, vit_model=\"vit_b16\"):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.num_classes = num_classes\n",
    "        self.vit = vit_model.from_pretrained(vit_model)\n",
    "        self.linear = nn.Linear(self.vit.embed_dim, self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vit(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, masks_1, masks_2 = batch\n",
    "        output = self(images)\n",
    "        loss = F.cross_entropy(output, masks_1)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, masks_1, masks_2 = batch\n",
    "        output = self(images)\n",
    "        loss = F.cross_entropy(output, masks_1)\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## modèle et fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.fit(model, dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have loaded your data into these variables\n",
    "\n",
    "train_images = images[:60000,:,:]\n",
    "train_masks = masks[:60000,:,:,:]\n",
    "val_images = images[60000:75000,:,:]\n",
    "val_masks = masks[60000:75000,:,:,:]\n",
    "test_images = images[75000:,:,:]\n",
    "test_masks = masks[75000:,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_module = DataModule(train_images, train_masks, val_images, val_masks, test_images, test_masks)\n",
    "model = InstanceSegmentationModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Trainer and train your model\n",
    "trainer = pl.Trainer(max_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "       # PyTorch Transformer \n",
    "        d_model = 52 #512\n",
    "        nhead = 4\n",
    "        dim_feedforward = 2048 \n",
    "        dropout = 0.1\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)  \n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "        \n",
    "        # Segmentation head\n",
    "        self.segmentation_head = nn.Sequential(\n",
    "           nn.Conv2d(16, 1, kernel_size=1),\n",
    "           nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "           nn.ReLU(),\n",
    "           nn.Conv2d(64, 2, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        # Other init steps\n",
    "        self.loss_fn = nn.BCELoss()\n",
    "    \n",
    "   \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        feats = self.encoder(x) \n",
    "        \n",
    "        preds = self.decoder(feats, memory=feats)\n",
    "        preds = self.segmentation_head(preds)\n",
    "        return preds\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        print(f\"Epoch {self.current_epoch} done. Average Train Loss: {avg_loss.item()}\")\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Validation step\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log('val_loss', loss)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Test step\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "train_dataset = SegmentationDataset(images[:50000], masks[:50000]) \n",
    "val_dataset = SegmentationDataset(images[50000:70000], masks[50000:70000])\n",
    "test_dataset = SegmentationDataset(images[70000:], masks[70000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16) \n",
    "test_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SegmentationModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
